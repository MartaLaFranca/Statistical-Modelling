---
title: "La Franca Statistical Modeling"
author: "Marta La Franca (Matr. 866590)"
date: "29/4/2021"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Vengono caricate le librerie utili ai fini dello svolgimento del problema in esame

```{r message = FALSE}
library(car)
library(olsrr)
library(skedastic)
library(psych)
library(lmtest)
library(systemfit)
library(sandwich)
library(describedata)
library(klaR)
library(DataCombine)
library(pander)
library(lmtest)
```


Si carica la funzione white.test che verrà utilizzata durante lo studio dell'eteroschedasticità:

```{r message=FALSE}
white.test<-function(lmod){
  u2<-lmod$residuals^2
  y<-lmod$fitted
  R2u<-summary(lm(u2~y+I(y^2)))$r.squared
  LM<-length(y)*R2u
  p.val<-1-pchisq(LM,2)
  data.frame("Test Statistic"=LM, "P"=p.val)
}
```

## Analisi dei dati:

Si carica il dataset d'interesse:

```{r echo=TRUE}
data<-read.csv("C:/Users/marta/Downloads/sm_esame290421.csv")
```

Si effettua un print delle prime 6 righe del dataset: 

```{r}
pander(head(data),big.mark=",")
```

Si considerano le variabili numeriche: 

```{r}
var_num<-c("x1","x2","x3","y")
```

Si ordinano i dati secondo la variabile "time":

```{r}
data<-data[order(data$time),]
```

## Statistiche descrittive

```{r message=TRUE}
pander(summary(data[,var_num]))
```
E' possibile osservare che: le variabili presentano range diversi tra loro; la variabile $x1$ presenta un range da -4.440 a 24.603, mentre la variabile  $x3$ presenta un range da 3.281 a 3699.064.

Si può dedurre che nessuna variabile presenta dei valori nulli e/o anomali, e.g. un valore −999 che identifica missing value.

La variabile $x2$ presenta una media di 0.01307; la variabile $x3$ di 276.579.

Si descrivono le correlazioni delle variabili numeriche: si considerano in particolare istogrammi, scatter plot e il coefficiente di correlazio delle coppie di variabili

```{r}
pairs.panels(data[,-1])
```

E' possibile osservare che: la maggior correlazione è presente tra le variabili $x1$ e $x3$.

La variabile $x3$ presenta una coda a destra, mentre per le altre variabili non si distinguono particolari andamenti.

Tra le variabili $x3$ e $y$ e le variabili $x1$ e $y$ è presente una correlazione negativa, sebbene essa sia di lieve entità.

Non sembrano esserci dunque particolari correlazioni tra nessuna delle variabili considerate, solitamente infatti da questa matrice di correlazione le variabili che rappresentano delle problematiche sono considerate quelle con indici di correlazione superiori a 0,90.


Si effettuano i boxplot delle variabili numeriche: 

```{r}
par(mfrow=c(2,2)) 
for(i in var_num){
  boxplot(data[,i],main=i,col="light blue",ylab=i)     
}
```

Dall'analisi del boxplot si osserva che la variabile $x3$ presenta svariati outliers, e non presenta una distribuzione normale.

Per quanto riguarda le altre variabili numeriche invece si identifica una distribuzione quasinormale (a occhio) che risulta essere un buon punto di partenza, in quanto permettono di assumere le ipotesi del Teorema del Limite Centrale senza troppi problemi.

## Modello lineare

Viene adesso svolto il  modello lineare di y (variabile dipendente) su x1, x2 e log(x3) (variabili indipendenti): in particolare il test di ipotesi ed interpretazione dei coefficienti.

```{r}
mod1<-lm(y~x1+x2+I(log(x3)),data)
```

```{r}
pander(summary(mod1))
```
Effettuando la summary del modello si osserva che il modello risulta essere statisticamente significativo (presenta un p-value pari a 3.162e-16).

La variabile $x2$ è l'unica variabile, oltre l'intercetta ad essere statisticamente significativa ed ha un impatto positivo sulla $y$.

La variabile log(x3) invece ha un impatto negativo rispetto la $y$.

Il coefficiente $R^2$ risulta discretamente basso (0.3183); il modello ha dunque una capacità di adattamento bassa, ma non risulta anomalo in quanto solamente una variabile risulta statisticamente significativa. 

### Multicollinearità

Si effettua lo studio della multicollinearità utilizzando l'indice VIF e il condition index:

```{r}
pander(vif(mod1)) 
```
```{r}
pander(ols_eigen_cindex(mod1))
```
Analizzando i risultati si osserva che la variabile $x1$ e la variabile log($x3$) presentano dei VIF elevati; sono infatti considerati valori anomali dei valori maggiori di 10.

Inoltre considerando il condition index, l'autovalore caratterizzato da un condition index più elevato spiega il 96% della varianza di $x1$ e il 99% della varianza di log($x3$).

Dunque è possibile dire che vi sia collinearità tra le due variabili. 

Si decide dunque di proseguire nel modello eliminando una delle due variabili, ed in particolare si decide di eliminare la variabile log($x3$)

Si definisce un secondo modello:

```{r}
mod2<-lm(y~x1+x2,data)
```

```{r}
pander(summary(mod2))
```

Il modello così composto risulta, come prima, statisticamente significativo. 

In particolare adesso tutte le variabili sono statisticamente significative, come ci si poteva aspettare avendo eliminato la collinearità.

Il coefficiente $R^2$ invece risulta discretamente basso come in precedenza.

Si controlla la collinearità per verificare che si siano risolti i problemi: 

```{r}
pander(vif(mod2))
```
```{r}
pander(ols_eigen_cindex(mod2))
```
Come è possibile osservare è stato risolto il problema della multicollinearità.


### Omoschedasticità

Si effettua ora lo studio dell'omoschedasticità

Si effettua l'**analisi grafica**:

```{r}
par(mfrow=c(2,2))

plot(fitted(mod2),rstudent(mod2),ylab="rstand", xlab='fitted')
abline(h=2)
abline(h=-2)

plot(data$x1,rstudent(mod2), ylab='rstand', xlab='x1')    
abline(h=2)
abline(h=-2)

plot(data$x2,rstudent(mod2), ylab='rstand', xlab='x2')    
abline(h=2)
abline(h=-2)

plot(mod2$fitted, (mod2$residuals)^2, xlab='fitted',ylab='residuals sq. ')
```

Analizzando i grafici dei residui standardizzati vs i fitted, e dei residui vs le variabili esplicative, così come considerando i residui al quadrato non si rilevano particolari pattern che possano indicare la presenza di eteroschedasticità nel modello.

Tuttavia è fondamentale effettuare anche il **white test** per verificare analiticamente tale osservazione:

```{r}
pander(white.test(mod1),big.mark=',') 
```

Il test di White per l’omoschedasticità risulta nella regione di accettazione: non si rifiuta l’ipotesi di omoschedasticità dei residui. 

Non si ritiene dunque necessario apportare alcun tipo di modifica, come ci si aspettava dall'analisi grafica.

### Normalità dei residui

Si consideri il **QQ plot**

```{r}
plot(mod2,which=2)
```

Il QQPlot mostra un andamento irregolare solo sulle code: all’inizio e alla fine i punti si discostano dalla distribuzione teorica, tale andamento potrebbe essere dovuto alla presenza di outliers.

Si effettuano i **test per la normalità**

```{r}
ols_test_normality(mod2)
```
Il test di Shapiro-Wilks risulta nella regione di rifiuto dell'ipotesi nulla di normalità dei residui, tuttavia si decide di proseguire nell'analisi delle ipotesi, per verificare la presenza di eventuali valori anomali, considerando anche che n>25 dunque è possibile ricondursi al teorema del limite centrale per cui gli errori standard sono calcolati asintoticamente, dunque si decide di non realizzare correzioni in merito.

### Outlier

Si considerino eventuali outliers:

```{r}
n=length(fitted(mod2))
k=length(coef(mod2))

par(mfrow=c(2,2))

plot(hatvalues(mod2),rstudent(mod2),ylab='rstudent',xlab='leverage') 
abline(h=2)
abline(h=-2)
abline(h=0)
abline(v=2*k/n)
text(hatvalues(mod2),rstudent(mod2))

plot(cooks.distance(mod2),type='h')   
abline(h=4/n)
text(cooks.distance(mod2))

plot(dffits(mod2),type='h')
abline(h=2*sqrt(k/n))
abline(h=-2*sqrt(k/n))

plot(covratio(mod2))
abline(h=1+3*(k/n))
abline(h=1-3*(k/n))
text(covratio(mod2))
```

Si rileva la presenza di diversi outliers, in particolare si notano i valori con indice 94, 79, 100.

Si decide dunque di realizzare un dataset *pulito* eliminando i valori anomali:

```{r}
data2<-data[hatvalues(mod2)<2*k/n & abs(rstudent(mod2))<2 & cooks.distance(mod2)<4/n,]
mod_out<-lm(y~x1+x2,data2)
pander(summary(mod_out))
```
Si osserva che tutte le variabili risultano significative.

```{r}
ols_test_normality(mod_out)
```
E' stato risolto anche il problema della normalità.

## Autocorrelazione

Si effettua l'**analisi grafica**:

```{r}
par(mfrow=c(2,2))

plot(data$time,data$y, ylab='y',xlab='time',type='b')        
abline(h=mean(data$y))

acf(data$y, main="autocorr. y")

pacf(data$y, main="autocorr. parziale y")
```

Analzzando i grafici si rileva la presenza di autocorrelazione, in primo luogo viene considerata autocorrelazione di primo ordine.

Si effettua così il *Test di Durbin-Watson* per verificare analiticamente l'autocorrelazione:

```{r}
durbinWatsonTest(mod2,max.lag=8)
```
Si rileva la presenza di autocorrelazione, infatti viene respinta l'ipotesi nulla di incorrelazione, e si verifica che la stastitica presenta valori < 1 che indicano la presenza di autocorrelazione positiva.

Si procede prima di tutto considerando autocorrelazione di primo ordine.

### Risoluzione

Si decide di utilizzare la **procedura di Cochrane Orcutt** in cui si effettua una stima di ρ, e successivamente si effettuano le stime GLS.

Vengono creati i residui ritardati: 

```{r message=FALSE}
data$u_hat<-mod2$residuals
data<-slide(data=data, Var='u_hat',TimeVar= 'time', NewVar='u_hat_lag') 
```

Si procede effettuando la regressione di  u_hat su u_hat_lag

```{r}
aux<-lm(u_hat~u_hat_lag,data)
pander(summary(aux))
```
La stima di u_hat_lag è 0.8426

Si memorizza il coefficiente di autocorrelazione rho:

```{r}
rho<-aux$coefficients[2]
rho 
```

Vengono costruite le variabili laggate:

```{r message=FALSE}
data<-slide(data=data, Var='y',TimeVar= 'time', NewVar='y_lag')  
data<-slide(data=data, Var='x1',TimeVar= 'time', NewVar='x1_lag') 
data<-slide(data=data, Var='x2',TimeVar= 'time', NewVar='x2_lag') 
```

Si creano le variabili trasformate: 

```{r}
data$y_t<-data$y-rho*data$y_lag
data$x1_t<-data$x1-rho*data$x1_lag
data$x2_t<-data$x2-rho*data$x2_lag
data$interc_t<-1-rho
```

Si stima il modello con le variabili trasformate: 

```{r}
mod3<-lm(y_t~0+interc_t+x1_t+x2_t,data)
pander(summary(mod3))
```
```{r}
pander(summary(mod2))
```
Si osserva che la bontà di adattamento del modello risulta ampiamente migliorata, il modello adesso presenta un'ottima bontà di adattamento ($R^2$ = 0.9468)

Il modello come prima è significativo.

Le variabili rimangono stasticamente significative. 

Si effettua il test di Durbin Watson per controllo:

```{r}
durbinWatsonTest(mod3, max.lag = 5)
```
Si osserva che l'autocorrelazione è stata risolta. 





